{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 1.8.0\n",
      "pytorch 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, json, random\n",
    "import time\n",
    "from importlib import reload\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow\", tf.__version__)\n",
    "\n",
    "import torch\n",
    "print(\"pytorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional dependencies (install with `conda`):\n",
    "- `ftfy`\n",
    "- `spacy`\n",
    "\n",
    "Also download spaCy `en` model with:\n",
    "```\n",
    "python -m spacy download en\n",
    "```\n",
    "(see https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftfy 5.4.1\n",
      "spacy 2.0.11\n"
     ]
    }
   ],
   "source": [
    "import ftfy\n",
    "print(\"ftfy\", ftfy.__version__)\n",
    "import spacy\n",
    "print(\"spacy\", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text and apply BPE\n",
    "\n",
    "The `TextEncoder` object handles tokenization and applying BPE to raw text, giving a list of IDs. See https://github.com/openai/finetune-transformer-lm/blob/master/text_utils.py and https://github.com/openai/finetune-transformer-lm/blob/master/utils.py#L14\n",
    "\n",
    "In order to align with original text tokenization, we probably want a two-stage process:\n",
    "1. Recover the processed spaCy tokens by a reverse lookup, and project annotations to spaCy tokenization.\n",
    "2. Project annotations from the spaCy tokenization to the BPE pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.openai_transformer_lm.utils' from '/nfs/jsalt/home/iftenney/jiant/src/openai_transformer_lm/utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.openai_transformer_lm import utils as openai_utils\n",
    "reload(openai_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[718, 889, 2510, 636, 246, 8210, 7961, 7961]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"how much wood would a woodchuck chuck\"\n",
    "e = openai_utils.encode([text])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['how</w>',\n",
       " 'much</w>',\n",
       " 'wood</w>',\n",
       " 'would</w>',\n",
       " 'a</w>',\n",
       " 'wood',\n",
       " 'chuck</w>',\n",
       " 'chuck</w>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_utils.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_dir /nfs/jsalt/home/iftenney/jiant/src/openai_transformer_lm\n",
      "openai_data_dir /nfs/jsalt/home/iftenney/jiant/src/openai_transformer_lm/tf_original/model\n"
     ]
    }
   ],
   "source": [
    "openai_dir = os.path.dirname(openai_utils.__file__)\n",
    "print('openai_dir', openai_dir)\n",
    "openai_data_dir = openai_utils.openai_data_dir\n",
    "print('openai_data_dir', openai_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['how</w>',\n",
       "  'much</w>',\n",
       "  'wood</w>',\n",
       "  'would</w>',\n",
       "  'a</w>',\n",
       "  'wood',\n",
       "  'chuck</w>',\n",
       "  'chuck</w>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(openai_utils.decode_partial(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how much wood would a woodchuck chuck']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(openai_utils.decode_full(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For their model, they process the ID list by adding start and end IDs, `encoder['_start_']` and `encoder['_delimiter_']`, then pad with `clf_token = encoder['_classify_']`. All of these are set equal to `n_vocab = len(encoder)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a</w>', 'recent</w>', 'report</w>']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_utils.N_VOCAB\n",
    "list(openai_utils.decode_partial([[246, 6264, 4144]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from src import retokenize\n",
    "\n",
    "fname = \"/nfs/jsalt/share/glue_data/edges/spr2/train.edges.json\"\n",
    "records = list(utils.load_json_data(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_tokenize_with_eow(sentence):\n",
    "    \"\"\"Add </w> markers to ensure word-boundary alignment.\"\"\"\n",
    "    return [t + \"</w>\" for t in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenAligner(7, 8):\n",
      "  0 -> [0]\n",
      "  1 -> [1]\n",
      "  2 -> [2]\n",
      "  3 -> [3]\n",
      "  4 -> [4]\n",
      "  5 -> [5, 6]\n",
      "  6 -> [7]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "text = \"how much wood would a woodchuck chuck\"\n",
    "ta = retokenize.TokenAligner(space_tokenize_with_eow(text), openai_utils.tokenize(text))\n",
    "print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenAligner(17, 19):\n",
      "  0 -> [0]\n",
      "  1 -> [1]\n",
      "  2 -> [2]\n",
      "  3 -> [3, 4]\n",
      "  4 -> [5]\n",
      "  5 -> [6]\n",
      "  6 -> [7]\n",
      "  7 -> [8]\n",
      "  8 -> [9]\n",
      "  9 -> [10]\n",
      "  10 -> [11]\n",
      "  11 -> [12, 13]\n",
      "  12 -> [14]\n",
      "  13 -> [15]\n",
      "  14 -> [16]\n",
      "  15 -> [17]\n",
      "  16 -> [18]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "record = records[10]\n",
    "text = record['text']\n",
    "ta = retokenize.TokenAligner(space_tokenize_with_eow(text), openai_utils.tokenize(text))\n",
    "print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i</w>',\n",
       " 'have</w>',\n",
       " 'a</w>',\n",
       " 'pre',\n",
       " 'order</w>',\n",
       " 'and</w>',\n",
       " 'am</w>',\n",
       " 'even</w>',\n",
       " 'considering</w>',\n",
       " 'getting</w>',\n",
       " 'a</w>',\n",
       " 'second</w>',\n",
       " 'pre',\n",
       " 'order</w>',\n",
       " 'to</w>',\n",
       " 'have</w>',\n",
       " 'multiple</w>',\n",
       " 'accounts</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_utils.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI TensorFlow Model\n",
    "\n",
    "Adapted from https://github.com/openai/finetune-transformer-lm/blob/master/train.py#L163 to just export weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /nfs/jsalt/home/iftenney/jiant/src/openai_transformer_lm/transformer_tf_simplified.py:51: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Running initializer...\n",
      "Loading pre-trained params...\n",
      "Assigning pre-trained params...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from src.openai_transformer_lm import transformer_tf_simplified\n",
    "reload(transformer_tf_simplified)\n",
    "from src.openai_transformer_lm.tf_original import utils as openai_tf_utils\n",
    "assert openai_utils.N_VOCAB == transformer_tf_simplified.n_vocab\n",
    "\n",
    "SHAPES_FILE = os.path.join(openai_data_dir, \"params_shapes.json\")\n",
    "PARAMS_FILE_TMPL = os.path.join(openai_data_dir, \"params_{}.npy\")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    X_in = tf.placeholder(tf.int32, [None, transformer_tf_simplified.n_ctx, 2])\n",
    "    h = transformer_tf_simplified.model_abbreviated(tf.expand_dims(X_in, 1))\n",
    "    \n",
    "    params = openai_tf_utils.find_trainable_variables(\"model\")\n",
    "    print(\"Running initializer...\")\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    transformer_tf_simplified.load_params(sess, params, SHAPES_FILE, PARAMS_FILE_TMPL)\n",
    "    \n",
    "    h_val = sess.run(h, {X_in:openai_utils.prep_ids(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.16202518, -0.00763676,  0.14871871, ...,  0.17555687,\n",
       "         -0.0820915 ,  0.15243863],\n",
       "        [-0.22960284, -0.7131108 ,  0.25970727, ...,  0.29409307,\n",
       "         -0.28634194, -0.3005669 ],\n",
       "        [-0.33067906, -0.11411758,  0.37495106, ..., -0.23809306,\n",
       "         -1.2012712 , -0.27857336],\n",
       "        ...,\n",
       "        [ 0.50163245,  0.84288955,  0.8027246 , ..., -0.03076262,\n",
       "          1.1621311 ,  0.7543984 ],\n",
       "        [ 0.51369536,  0.7608172 ,  0.8307392 , ..., -0.05948838,\n",
       "          1.0978718 ,  0.6896318 ],\n",
       "        [ 0.47562498,  0.79977787,  0.8108253 , ..., -0.0525886 ,\n",
       "          1.1286504 ,  0.7206507 ]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface PyTorch port\n",
    "\n",
    "Code from https://github.com/huggingface/pytorch-openai-transformer-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "from src.openai_transformer_lm.pytorch_huggingface import model_pytorch\n",
    "reload(model_pytorch)\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "args = model_pytorch.DEFAULT_CONFIG\n",
    "n_special = transformer_tf_simplified.n_special\n",
    "model = model_pytorch.TransformerModel(args, vocab=40990+n_special)\n",
    "loader_args = dict(n_special=n_special)\n",
    "loader_args['path'] = openai_data_dir + \"/\"\n",
    "loader_args['path_names'] = os.path.dirname(model_pytorch.__file__) + \"/\"\n",
    "model_pytorch.load_openai_pretrained_model(model, **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_embd': 768,\n",
       " 'n_head': 12,\n",
       " 'n_layer': 12,\n",
       " 'embd_pdrop': 0.1,\n",
       " 'attn_pdrop': 0.1,\n",
       " 'resid_pdrop': 0.1,\n",
       " 'afn': 'gelu',\n",
       " 'clf_pdrop': 0.1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(16).repeat(4,1)\n",
    "t.size()[0]\n",
    "torch.stack([t,t], dim=2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ids = torch.LongTensor(openai_utils.prep_ids(e))\n",
    "h_val_pytorch = model(ids).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_val_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.1620266 , -0.00763825,  0.14871885, ...,  0.17555933,\n",
       "         -0.08209433,  0.15244102],\n",
       "        [-0.22960448, -0.71311027,  0.25970703, ...,  0.2940926 ,\n",
       "         -0.28634158, -0.3005667 ],\n",
       "        [-0.33067977, -0.11411723,  0.37495226, ..., -0.23809452,\n",
       "         -1.2012721 , -0.2785728 ],\n",
       "        ...,\n",
       "        [ 0.50163686,  0.8428891 ,  0.80272746, ..., -0.03076063,\n",
       "          1.1621205 ,  0.7543992 ],\n",
       "        [ 0.51369643,  0.76081246,  0.8307426 , ..., -0.05948911,\n",
       "          1.0978615 ,  0.6896316 ],\n",
       "        [ 0.47562388,  0.7997696 ,  0.81082684, ..., -0.05258931,\n",
       "          1.1286354 ,  0.7206483 ]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_val_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0493553e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((h_val - h_val_pytorch)**2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Looks like the PyTorch implementation loads the weights correctly and matches the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllenNLP Implementation\n",
    "\n",
    "See code here: https://github.com/allenai/allennlp/blob/master/allennlp/modules/openai_transformer.py\n",
    "\n",
    "This is ported from the Huggingface implementation and looks a bit cleaner, but it isn't immediately clear how to use it - it expects a tarfile containing the weights, but it's not clear how to generate this. We'd also need to update AllenNLP, which might break other experiments using `jiant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
