// Config settings used for SuperGLUE BERT baseline experiments.

// This imports the defaults, which can be overridden below.
include "../defaults.conf"
exp_name = "taskmaster"

// Data and preprocessing settings
max_seq_len = 256 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.

// Model settings
input_module = "roberta-large"
pytorch_transformers_output_mode = "top"
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune // finetune entire BERT model

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 4
max_epochs = 10
lr = .00001
min_lr = .0000001
lr_patience = 10000 // disable lr annealing on plateau
patience = 20
max_vals = 10000
val_interval = 5000
target_train_val_interval = 5000

// Control-flow stuff
do_pretrain = 1
do_target_task_training = 1
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1

# Task-specific stuff
copa {
    val_interval = 100
}
rte-superglue {
    val_interval = 625
}
rte {
    val_interval = 625
}
boolq {
    val_interval = 2400
}
commitbank {
    val_interval = 60
}
multirc {
    val_interval = 1000
}
wic {
    val_interval = 1000
}
winograd-coreference {
    val_interval = 130
}
commonsenseqa {
    val_interval = 2500
}
acceptability-conj {
    val_interval = 91
}
acceptability-def {
    val_interval = 101
}
acceptability-eos {
    val_interval = 95
}
acceptability-wh {
    val_interval = 416
}
