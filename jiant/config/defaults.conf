// Default config file.
// Set values here, then override them in custom configs by including this at
// the top:
//
//   my_experiment.conf:
//     include "defaults.conf"
//
//     exp_name = my_expt
//     run_name = run1
//     pretrain_tasks = "mnli,qnli"
//
// ... or in a command line flag:
//
//   $ python main.py --config_file jiant/config/defaults.conf \
//         --overrides "exp_name = my_expt, run_name = run1, pretrain_tasks = \"mnli,qnli\""
//
// This file uses HOCON, which is a JSON/YAML-like format that supports
// includes, references, and object merging semantics. For reference, see:
//
// https://github.com/lightbend/config/blob/master/HOCON.md


// Misc. Logistics //

cuda = 0  // GPU ID. Set to -1 for CPU. On machines without GPUs, this is ignored.
random_seed = 1234  // Global random seed, used in both Python and PyTorch random number generators.
track_batch_utilization = 0  // Track % of each batch that is padding tokens (for tasks with field
                             // 'input1').


// Paths and Logging //

// You'll generally have to override these:
project_dir = ${JIANT_PROJECT_PREFIX}  // The base directory for model output.
exp_name = my-experiment  // Experiment name, will be a subdirectory of project_dir. This
                          // directory will contain all run directories, a results summary file,
                          // and preprocessed data files.
run_name = tuning-0  // Run name, will be a subdirectory of exp_name. This directory will contain
                     // logs, checkpoints, and model predictions.
data_dir = ${JIANT_DATA_DIR}  // Base directory in which to look for raw data subdirectories. This
                              // could be the glue_data directory created by download_glue_data.py.

// You may want to override these:
global_ro_exp_dir = ""  // If you're using very large datasets for which preprocessing
                        // is slow, you can set this to point to a directory for a past experiment
                        // (different from the current one), and the 'preproc' index files will
                        // be read from that directory to save time. If this directory does not
                        // exist, all data will be preprocessed as usual without failing.
remote_log_name = ${exp_name}"__"${run_name}  // Log name for GCP remote logging, if used. This
                                              // should be globally unique to your run. Usually
                                              // safe to ignore.

// You can safely ignore these, and they'll be set automatically:
exp_dir = ${project_dir}"/"${exp_name}"/"
run_dir = ${project_dir}"/"${exp_name}"/"${run_name}
local_log_path = ${run_dir}"/log.log"
list_params = 1  // Before training, print a list of all trainable model parameters with their
                 // shapes. Takes up a lot of log space, but useful for debugging.


// Tasks //

pretrain_tasks = sst  // Comma-separated list of pretraining tasks or 'glue' or 'superglue' or 'none'.
                      // If there are multiple entries, the list should contain no spaces, and
                      // should be enclosed in quotes. When using command line overrides, you need
                      // to escape these quotes:
                      //   python main.py --overrides "pretrain_tasks = \"glue,ccg\""
                      // Note: The code expects this to be nonempty in most cases. If you want to
                      // train and evaluate on a single task without doing any new pretraining,
                      // you should set target_tasks and pretraining_tasks to the same task, set
                      // do_pretrain to 1, and do_target_task_training to 0.
target_tasks = glue  // Target tasks, for use in both target_task_training
                     // (if do_target_training = 1) and the final evaluation,
                     // (if do_full_eval = 1), and is in the same list format as pretrain_tasks.


// Execution, Saving, and Loading //

// Three main stages of operation
do_pretrain = 1  // Run training on the tasks in pretrain_tasks.
do_target_task_training = 1  // After do_pretrain, train on the target tasks in target_tasks.
do_full_eval = 1  // Evaluate on the tasks in target_tasks.

// Related configuration
load_model = 1  // If true, restore from checkpoint when starting do_pretrain. No impact on
                // do_target_task_training.
transfer_paradigm = "frozen" // How to use pretrained model parameters during target task training.
                             // Applies to GPT, BERT, and pretrained sent_enc encoders.
                             // Options: "frozen", "finetune"
                             //   "frozen" will train the downstream models on fixed
                             //   representations from the encoder model.
                             //   "finetune" will update the parameters of the encoders models as
                             //    well as the downstream models. (This disables d_proj.)
load_target_train_checkpoint = none  // If not "none", load the specified model_state checkpoint
                                     // file when starting do_target_task_training.
                                     // Supports * wildcards.
load_eval_checkpoint = none  // If not "none", load the specified model_state checkpoint
                             // file when doing do_full_eval.
                             // Supports * wildcards.

allow_untrained_encoder_parameters = 0  // Set for experiments involving random untrained sent_enc
                                        // encoders only. Allows do_target_task_training and
                                        // do_full_eval to proceed without pretraining.
allow_reuse_of_pretraining_parameters = 0  // Set to 1 to allow task models that were trained
                                           // during pretraining to be reused in
                                           // do_target_task_training. This may behave incorrectly
                                           // if a run is stopped and restarted in
                                           // do_target_task_training (issues #285, #290).
allow_missing_task_map = 0  // Dangerous: If true, ignore missing classifier_task_map.json
                            // Only relevant to runs that use ELMo. This is needed for bare-ELMo
                            // probing, since the pretraining phase is skipped for these models.
reload_tasks = 0  // If true, force the rebuilding of the task files in the experiment directory,
                  // even if they exist.
reload_indexing = 0  // If true, force the rebuilding of the index files in preproc/ for tasks in
                     // reindex_tasks, even if they exist.
reindex_tasks = ""  // See reload_indexing above.
reload_vocab = 0     // If true, force the rebuilding of the vocabulary files in the experiment
                     // directory. For classification and
                     // regression tasks with the default ELMo-style character handling, there
                     // is no vocabulary.

// Learning curves
target_train_data_fraction = 1  // Use only the specified fraction of the training data in the
                                // do_target_task_training phase. Should not impact pretraining,
                                // even for the same task.
               // If target_train_data_fraction and pretrain_data_fraction are equal,
               // then the training set for pretraining and target tasks will be the same.
pretrain_data_fraction = 1  // Use only the specified fraction of the training data in the
                            // do_pretrain phase.  Should not impact target-phase training, even for
                            // the same task.
                            // Note: This uses rejection sampling at training time, so it can slow
                            // down training for small fractions (<5%).


// Training options //

// Optimization
trainer_type = sampling  // Type of trainer object. Currently only one option: 'sampling'
batch_size = 32  // Training batch size.
optimizer = adam  // Optimizer. All valid AllenNLP options are available, including 'sgd'.
                  // Use 'bert_adam' for reproducing BERT experiments.
                  // 'adam' uses the newer AMSGrad variant.
                  // Warning: bert_adam is designed for cases where the number of epochs is known
                  // in advance, so it may not behave reasonably unless max_epochs is set to a
                  // reasonable positive value.
lr = 0.0001  // Initial learning rate.
min_lr = 0.000001  // Minimum learning rate. Training will stop when our explicit LR decay lowers
                   // the LR below this point or if any other stopping criterion applies.
max_grad_norm = 5.0  // Maximum gradient norm, for use in clipping.
lr_patience = 1  // Patience to use (in validation checks) before decaying the learning rate.
                 // Learning rate will decay after lr_patience + 1 validation checks have
                 // completed with no improvement in validation score.
lr_decay_factor = 0.5  // Factor by which to decay LR (multiplicative) when lr_patience is reached.
scheduler_threshold = 0.0001  // Threshold used in deciding when to lower learning rate.

// Validation, Checkpointing, and Early Stopping
val_data_limit = 5000  // Maximum number of examples to be used during mid-training validations.
                       // We use the _first_ N (e.g., 5000) examples from each dev set. Does not
                       // apply to the final validation run at the end of main.py that is invoked
                       // by do_full_eval.
val_interval = 1000  // Number of gradient steps to take between validation checks. Note that
                     // the stopping criteria are only checked at these intervals.

max_vals = 1000  // Maximum number of validation checks. Will stop once this limit has been
                 // reached. This cannot be disabled, but if you plan to rely on max_epochs or
                 // min_lr instead for stopping training, simply set it to a very high number.
max_epochs = -1 // If positive, maximum number of epochs (full pass over a task's training data)
                // to train for. Training will stop once it hits max_epochs for any task or hits
                // or any other stopping criterion (max validations, minimum learning). We only
                // check this criterion when doing validation, so if val_interval is too high,
                // especially if it's higher than one epoch's worth of steps, it's possible to
                // significantly overshoot the intended number of epochs.

patience = 5  // Patience in early stopping. Training will stop if performance does not improve at
              // all in patience + 1 validations.
keep_all_checkpoints = 0  // If set, keep checkpoint files from every validation. Otherwise, keep
                          // only the best and (if different) most recent.
delete_checkpoints_when_done = 0  // If set, delete *all* saved checkpoints when the run is
                                  // complete. Be careful when using this, but it can be useful in
                                  // cases where checkpoints are large (as with BERT), runs are
                                  // fast, and you're confident that you won't need to run further
                                  // tests on any specific trained model. Will not apply if
                                  // keep_all_checkpoints is set.

// Multi-task Training
weighting_method = proportional  // Weighting method for task sampling, relative to the number of
                                 // training examples in each task:
                                 // Options: uniform, power_<power>, softmax_<temp>
                                 //   proportional, proportional_log_batch, and
                                 //   proportional_log_example (plus the less-useful inverse,
                                 // inverse_log_example, and inverse_log_batch).
                                 // See relevant source code for details.
scaling_method = uniform  // Method for scaling loss:
                          // Options: uniform, max_power_<power>, max_proportional,
                          //   max_proportional_log, max_inverse, max_inverse_log
                          //   max_epoch_<E1_E2_..._En>
                          // See relevant source code for details.
dec_val_scale = 250  // when training with increasing and decreasing val metrics
                     // (or multiple decreasing metrics), we use the macro average
                     // for early stopping, where decreasing metrics are aggregated
                     // as (1 - metric / dec_val_scale).
                     // Currently, perplexity is our only decreasing metric.

// Evaluation
write_preds = 0  // 0 _or_ comma-separated list of splits (without spaces, options are train, val,
                 // or test) for which we should write predictions to disk during do_full_eval.
                 // Supported for GLUE tasks and a few others. You should see errors with
                 // unsupported tasks.
write_strict_glue_format = 0  // If true, write_preds will only write the 'index' and 'prediction'
                              // columns for GLUE/SuperGLUE tasks, and will use the test filenames
                              // expected by the GLUE evaluation server.


// Preprocessing //

max_seq_len = 40  // Maximum sequence length, in tokens (usually in full tokens, even for
                  // models with char handling).
max_word_v_size = 30000  // Maximum input word vocab size, when creating a new embedding matrix.
                         // Not used for ELMo.
max_char_v_size = 250  // Maximum input char vocab size, when creating a new embedding matrix.
                       // Not used for ELMo.
max_targ_word_v_size = 20000  // Maximum target word vocab size for seq2seq tasks.


// Input Handling //

input_module = ""  // The word embedding or contextual word representation layer.
                   // Currently supported options:
                   //   - scratch: Word embeddings trained from scratch.
                   //   - glove: Loaded GloVe word embeddings. Typically used with
                   //       tokenizer = MosesTokenizer. Note that this is not quite identical to
                   //       the Stanford tokenizer used to train GloVe.
                   //   - fastText: Loaded fastText word embeddings. Use with
                   //      tokenizer = MosesTokenizer.
                   //   - elmo: AllenNLP's ELMo contextualized word vector model hidden states. Use
                   //       with tokenizer = MosesTokenizer.
                   //   - elmo-chars-only: The dynamic CNN-based word embedding layer of AllenNLP's
                   //       ELMo, but not ELMo's LSTM layer hidden states. Use with
                   //       tokenizer = MosesTokenizer.
                   //   - bert-base-uncased, etc.: Any BERT model from pytorch_transformers.
                   //   - roberta-base / roberta-large / roberta-large-mnli: RoBERTa model from
                   //       pytorch_transformers.
                   //   - xlnet-base-cased / xlnet-large-cased: XLNet Model from
                   //       pytorch_transformers.
                   //   - openai-gpt: The OpenAI GPT language model encoder from
                   //       pytorch_transformers.
                   //   - gpt2 / gpt2-medium / gpt2-large: The OpenAI GPT-2 language model encoder from
                   //       pytorch_transformers.
                   //   - transfo-xl-wt103: The Transformer-XL language model encoder from
                   //       pytorch_transformers.
                   //   - xlm-mlm-en-2048: XLM english language model encoder from
                   //       pytorch_transformers.
                   // Note: Any input_module from pytorch_transformers requires
                   // tokenizer = ${input_module} or auto.

tokenizer = auto  // The name of the tokenizer, passed to the Task constructor for
                  // appropriate handling during data loading. Currently supported
                  // options:
                  //   - auto: Select the tokenizer that matches the model specified in
                  //       input_module above. Usually a safe default.
                  //   - "": Split the input data on whitespace.
                  //   - MosesTokenizer: Our standard word tokenizer. (Support for
                  //       other NLTK tokenizers is pending.)
                  //   - bert-uncased-base, etc.: Use the tokenizer supplied with
                  //       pytorch_transformers that corresponds the input_module.
                  //   - SplitChars: Splits the input into individual characters.

word_embs_file = ${WORD_EMBS_FILE}  // Path to embeddings file, used with glove and fastText.
d_word = 300  //  Dimension of word embeddings, used with scratch, glove, or fastText.

elmo_weight_file_path = none  // Path to ELMo RNN weights file. Default ELMo weights will be used
                              // if "none".

char_embs = 0  // Experimental. If true, train char embeddings. This is separate from the ELMo char
               // component, and the two usually aren't used together.
d_char = 100  // Dimension of trained char embeddings.
n_char_filters = 100  // Number of filters in trained char CNN.
char_filter_sizes = "2,3,4,5"  // Size of char CNN filters.

pytorch_transformers_output_mode = "none"  // How to handle the embedding layer of the
                                           // BERT/XLNet model:
                                           // "none" or "top" returns only top-layer activation,
                                           // "cat" returns top-layer concatenated with
                                           //   lexical layer,
                                           // "only" returns only lexical layer,
                                           // "mix" uses ELMo-style scalar mixing (with learned
                                           //   weights) across all layers.
pytorch_transformers_max_layer = -1  // Maximum layer to return from BERT etc. encoder. Layer 0 is
                                     // wordpiece embeddings. pytorch_transformers_embeddings_mode
                                     // will behave as if the is truncated at this layer, so 'top'
                                     //  will return this layer, and 'mix' will return a mix of all
                                     // layers up to and including this layer.
                                     // Set to -1 to use all layers.
                                     // Used for probing experiments.

force_include_wsj_vocabulary = 0  // Set if using PTB parsing (grammar induction) task. Makes sure
                                  // to include WSJ vocabulary.


// Sentence Encoders on top of Input/Word Encoders //

sent_enc = rnn  // The type of sentence encoder we should use. This is part of the core jiant model,
                // while encoders like ELMo, GPT, and BERT are input handlers, so it's safe to set
                // this to 'none' when using ELMo and standard/expected to set this to 'none' when
                // using GPT or BERT.
                // Options: 'bow', 'rnn' (LSTM/BiLSTM), 'none'
                //   Specialized grammar induction options: 'prpn', 'onlstm'
                // Note: 'bow' just skips the encoder step and passes the word representations to
                // the task model, so it is possible to combine attention or max pooling with the
                // 'bow' encoder.
bidirectional = 1  // If true, the 'rnn' encoder (if used) should be bidirectional.
d_hid = 1024  // Hidden dimension size
n_layers_enc = 2  // Number of layers for a 'rnn' sent_enc.
skip_embs = 1  // If true, concatenate the sent_enc's input (ELMo/GPT/BERT output or
               // embeddings) with the sent_enc's  output.
sep_embs_for_skip = 0  // Whether the skip embedding uses the same embedder object as the original
                       // embedding (before skip).
                       // Only makes a difference if we are using ELMo weights, where it allows
                       // the four tuned ELMo scalars to vary separately for each target task.
n_layers_highway = 0  // Number of highway layers between the embedding layer and the sent_enc layer. [Deprecated.]
dropout = 0.2  // Dropout rate.
dropout_embs = ${dropout}  // Dropout rate for embeddings, same as above by default.
                           // NB: This only applies to trained char embs, not including ELMo.

cove = 0  // If true, use CoVe contextualized word embeddings. Should be used with
          // input_handler = glove.
cove_fine_tune = 0  // If true, CoVe params are fine-tuned.

onlstm_chunk_size = 2  // Chunk downsizing factor for ON-LSTM master gate, dimensions
                        // of master gate: D/C where C is the chunk downsizing factor.
onlstm_dropconnect = 0.5  // Linear dropout between ON-LSTM layers.
onlstm_dropouti  = 0.3  // Locked Dropout on input embeddings.
onlstm_dropouth  = 0.3  // Locked Dropout between ON-LSTM layers.
onlstm_tying = 0  // Language Modeling tying of weights.

n_slots = 15 // default PRPN-LM paper hyperparameter; the number of memory slots in reading network
n_lookback = 5 // default PRPN-LM paper hyperparameter; the lookback range of parsing network
resolution = 0.1 // default PRPN-LM paper hyperparameter; syntactic distance resolution
idropout = 0.5 // default PRPN-LM paper hyperparameter; dropout for layers
rdropout = 0.5 // default PRPN-LM paper hyperparameter; dropout for recurrent states
res = 0 // default PRPN-LM paper hyperparameter; number of res-net blocks in parser


// Task-specific Options //

// These are _usually_ overridden for specific tasks, and are explicitly overridden in this file
// for many tasks, but defaults are set here.

// Model
classifier = mlp  // The type of the final layer(s) in classification and regression tasks.
                  // Options:
                  //   log_reg: Softmax layer with no additional hidden layer.
                  //   mlp: One tanh+layernorm+dropout layer, followed by a softmax layer.
                  //   fancy_mlp: Same as mlp, but with an additional hidden layer. Fancy!
classifier_hid_dim = 512  // The hidden dimension size for mlp and fancy_mlp.
classifier_dropout = 0.2  // The dropout rate for mlp and fancy_mlp.
pair_attn = 1  // If true, use attn in sentence-pair classification/regression tasks.
d_hid_attn = 512  // Post-attention LSTM state size.
shared_pair_attn = 0  // If true, share pair_attn parameters across all tasks that use it.
d_proj = 512  // Size of task-specific linear projection applied before before pooling.
              // Disabled when fine-tuning pytorch_transformers models.
pool_type = "auto"  // Type of pooling to reduce sequences of vectors into a single vector.
                    // Options: "auto", "max", "mean", "first", "final"
                    // "auto" uses "first" for plain BERT (with no sent_enc), "final" for plain
                    // XLNet and GPT, and "max" in all other settings.
span_classifier_loss_fn = "softmax"  // Classifier loss function. Used only in some tasks (notably
                                     // span-related tasks), not mlp/fancy_mlp. Currently supports
                                     // sigmoid and softmax.
classifier_span_pooling = "attn"  // Span pooling type (see README section on edge probing-style
                                  // experiments for context).
                                  // Options: 'attn' or one of the 'combination' arguments accepted
                                  //   by AllenNLP's EndpointSpanExtractor such as 'x,y'.

edgeprobe_cnn_context = 0  // Expanded context for edge probing via CNN.
                           // 0 looks at only the current word, 1 adds +/-
                           // words (kernel width 3), etc.
edgeprobe_symmetric = 0    // If true, use same parameters for extracting span1
                           // and span2

// Training
target_train_val_interval = 500  // Comparable to val_interval, used during
                                 // do_target_task_training. Can be set separately per task.
target_train_max_vals = 1000  // Comparable to max_vals, used during do_target_task_training.
                              // Can be set separately per task.

// Evaluation
use_classifier = ""  // Used to make some task (usually a probing task with no training set) use a
                     // model that was trained for a different task at do_full_eval time. This
                     // should be overridden for each probing/auxiliary/test-only task, and set to
                     // the name of the task on which the associated model should be trained.


// Task-Specific Overrides //

// Note: Model params apply during all phases, but trainer params like LR apply only during eval
// phase.

glue-diagnostic { use_classifier = "mnli" }
broadcoverage-diagnostic { use_classifier = "rte-superglue" }
winogender-diagnostic { use_classifier = "rte-superglue" }

// Optional templates. These are tuned for simple non-transformer model.
// You can inherit from them using example lines like the ones commented out below.

glue-small-tasks-tmpl-1 {
    classifier_hid_dim = 128,
    d_proj = 128,
    classifier_dropout = 0.4,
    pair_attn = 0,
    val_interval = 100,
    lr = 0.0003
}

glue-small-tasks-tmpl-2 {
    classifier_hid_dim = 256,
    d_proj = 256,
    classifier_dropout = 0.2,
    pair_attn = 0,
    val_interval = 100,
    lr = 0.0003
}

glue-small-tasks-tmpl-3 {
    classifier_hid_dim = 512,
    classifier_dropout = 0.2,
    pair_attn = 1,
    val_interval = 1000,
    lr = 0.0003
}

s2s {
    d_hid_dec = 100
    n_layers_dec = 1
    // Options are 'Bahdanau' (a two-layer MLP to compute weights),
    // 'bilinear' (weights computed as x^T W y + b) or 'none'.
    attention = "Bahdanau"
    output_proj_input_dim = 100
    beam_size = 10
}

// rte = ${glue-small-tasks-tmpl-1}
// wnli = ${glue-small-tasks-tmpl-1}
// mrpc = ${glue-small-tasks-tmpl-2}
// sst = ${glue-small-tasks-tmpl-2}
// cola = ${glue-small-tasks-tmpl-2}
// sts-b = ${glue-small-tasks-tmpl-3}
// sts-b-alt = ${glue-small-tasks-tmpl-3}
// qnli = ${glue-small-tasks-tmpl-3}
// qnli-alt = ${glue-small-tasks-tmpl-3}
// mnli = ${glue-small-tasks-tmpl-3}
// mnli-alt = ${glue-small-tasks-tmpl-3}
// qqp = ${glue-small-tasks-tmpl-3}
// qqp-alt = ${glue-small-tasks-tmpl-3}

embeddings_train  = 0  // if set to 1, embeddings will be fine tuned.

nli-prob {
  probe_path = ""
}


// Edge-Probing Experiments //
// See README for context.

// Template: Not used for any single task, but extended per-task below.
edges-tmpl {
    span_classifier_loss_fn = "sigmoid"  // 'sigmoid' or 'softmax'
    classifier_span_pooling = "attn"  // 'attn' or 'x,y'
    classifier_hid_dim = 256
    classifier_dropout = 0.3
    pair_attn = 0

    // Default iters; run 50k steps.
    max_vals = 100
    val_interval = 500
}
edges-tmpl-small = ${edges-tmpl} {
    // 10k steps
    val_interval = 100
}
edges-tmpl-large = ${edges-tmpl} {
    // 250k steps
    max_vals = 250
    val_interval = 1000
}

edges-pos-ontonotes = ${edges-tmpl-large}
edges-nonterminal-ontonotes = ${edges-tmpl-large}
edges-ner-ontonotes = ${edges-tmpl-large}
edges-srl-ontonotes = ${edges-tmpl-large}
edges-coref-ontonotes = ${edges-tmpl-large}

edges-dep-ud-ewt = ${edges-tmpl-large}

edges-spr1 = ${edges-tmpl-small}
edges-spr2 = ${edges-tmpl-small}
edges-dpr = ${edges-tmpl-small}

edges-rel-semeval = ${edges-tmpl-small}
edges-rel-tacred = ${edges-tmpl}
