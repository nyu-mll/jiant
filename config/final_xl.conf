// Config for extra large final runs

include "final.conf"
exp_name = final // reuse 'final' experiment name so we don't have to reprocess stuff

// BIG preprocessing
max_seq_len = 40                // this will primarily affect MT
max_word_v_size = 20000         // these two params will affect all seq gen tasks
max_targ_word_v_size = 20000    // max_word_v_size is relevant to *English* generation tasks (LM, skipthought)
                                // max_targ_word_v_size is relevant to MT

// HUGE model params
d_hid =  1536                   // 1024 + 512 = 1536; 2048 is too HUGE
n_layers_enc = 2                // leave as is
dropout = 0.2                   // leave as is

// other options:
//   - classification / regression task: classifier_hid_dim, d_hid_attn, d_proj
//   - seq2seq: d_hid_dec, n_layers_dec

// ENORMOUS training params
batch_size = 32
task_patience = 4               // leave as is
patience = 20                   // leave as is
max_grad_norm = 5.0             // maybe a knob for regularization
