// This config is for STILTS training [https://arxiv.org/pdf/1811.01088.pdf]
// for BERT -> MNLI -> RTE
// For this example we use BERT-base.
// Run with:
//   python main.py --config_file config/examples/stilts_example.conf

include "../defaults.conf"
pretrain_tasks = "mnli"
target_tasks = "rte"

//Experiment configs
do_pretrain = 1      
do_target_task_training = 1 
do_full_eval = 1

batch_size = 24

write_preds = "val,test"

//BERT-specific parameters
bert_embeddings_mode = "top"    
bert_fine_tune = 1
sep_embs_for_skip = 1
sent_enc = "none"
classifier = log_reg // following BERT paper

dropout = 0.1 // following BERT paper
optimizer = bert_adam
max_epochs = 3
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000
transfer_paradigm = "finetune"

tokenizer = "bert-base-uncased"
bert_model_name = "bert-base-uncased"

