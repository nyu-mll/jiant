
include "cola-elmo.conf"


// Training setting

do_target_task_training = 0  // only do main training phase, since our pretrain and target task are the same
allow_reuse_of_pretraining_parameters = 1
batch_size = 2  // keep it small to avoid out-of-memory

// Model setting
elmo = 0
tokenizer = "OpenAI.BPE"
openai_transformer = 1
openai_embedding_mode = "none"
openai_transformer_fine_tune = 1
sent_enc = "null"
sep_embs_for_skip = 1  // otherwise some assert will refuse to build the model
lr = 1e-5  // we need to keep lr small for fine-tuning gpt, especially so because our BS is smaller than that in gpt paper
dropout = 0.1  // following gpt paper
