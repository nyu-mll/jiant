// Default config file.
// Set values here, then override them in custom configs by including this at
// the top, e.g.
// my_expt.conf:
//   include "defaults.conf"
//
//   exp_name = "my_expt"
//   run_name = "run1"
//   n_layers_env = 2
//   word_embs = "fastText"
//

// Logistics
cuda = 0  // gpu id, or -1 to disable
random_seed = 1234  // (global) random seed

// Paths and logging
// For local run, override this with:
//    in--overrides "project_dir = ${JIANT_PROJECT_PREFIX}",
// e.g. --overrides "project_dir = $HOME/exp"
// Please DO NOT change the default here! We want to use NFS for most
// experiments, unless your checkpoints are extremely large.
project_dir = ${NFS_PROJECT_PREFIX}  // for export to NFS
exp_name = "common-indexed-tasks"  // experiment name
run_name = "tuning-0"  // run name
exp_dir = ${project_dir}"/"${exp_name}"/"  // required
run_dir = ${project_dir}"/"${exp_name}"/"${run_name}  // required
local_log_path = ${run_dir}"/log.log"  // log file, goes in run directory
// log name for remote logging; make as unique as possible
remote_log_name = ${exp_name}"__"${run_name}
// Data paths
data_dir = ${JIANT_DATA_DIR}  // required
global_ro_exp_dir = "/nfs/jsalt/share/exp/default"  // saved preprocessing runs

// Execution control
do_train = 1    // run training steps
training_data_fraction = 1 // use only the specified fraction of the training data in the main training phase.
                           // should not impact eval-phase training, even for the same task.
do_eval = 1     // run eval steps
load_model = 1  // if true, restore main training from checkpoint if available
load_eval_checkpoint = "none"  // path to eval checkpoint, or "none"
allow_untrained_encoder_parameters = 0 // Set for experiments involving random untrained RNNs only.
skip_task_models = 0 // if true, only load task-independent parameters when using load_eval_checkpoint
reload_tasks = 0     // if true, force reloading tasks
reload_indexing = 0  // if true, force reindexing tasks for tasks in reindex_tasks
reindex_tasks = ""
reload_vocab = 0     // if true, force vocabulary rebuild
is_probing_task = 0

// Tasks and task-specific modules
train_tasks = sst // required, comma-separated list of training tasks,
                      // or 'glue' or 'none'
eval_tasks = glue // required, additional eval tasks
train_for_eval = 1  // if true and if needed, train new classifiers for eval tasks
classifier = mlp    // classifier type
classifier_loss_fn = ""  // classifier loss fn (only used by some tasks)
classifier_span_pooling = "x,y"  // span pooling type (for edge probing)
                                 // options are "attn" or one of the
                                 // 'combination' arguments accepted by
                                 // AllenNLP's EndpointSpanExtractor.
classifier_hid_dim = 512  // classifier width
classifier_dropout = 0.2  // classifier dropout rate
d_hid_dec = 300           // decoder hidden size
n_layers_dec = 1          // decoder number of layers
use_classifier = ""  // classifier for eval

// Preprocessing options
max_seq_len = 40         // maximum (input) sequence length
max_word_v_size = 30000  // maximum word vocab size
max_char_v_size = 250    // maximum char vocab size

// Embedding options
word_embs = "none"  // type of embeddings: 'none', 'scratch', 'glove, 'fastText'
word_embs_file = ${WORD_EMBS_FILE}  // path to embeddings file
                                    // shouldn't this be $FASTTEXT_EMBS_FILE ?
fastText = 0                                  // if true, use fastText model
fastText_model_file = ${FASTTEXT_MODEL_FILE}  // path to fastText model
d_word = 300  // dimension of word embeddings
d_char = 100  // dimension of char embeddings
n_char_filters = 100  // number of filters in char CNN
char_filter_sizes = "2,3,4,5"  // size of char filters
elmo = 1             // if true, use ELMo
elmo_chars_only = 1  // if true, use only char CNN layer of ELMo
cove = 0             // if true, use CoVe
char_embs = 0        // if true, use char embeddings
preproc_file = "preproc.pkl"  // file to save preprocessing data
                              // TODO(alex): document what goes in here?

// Model options
sent_enc = "rnn"  // type of sentence encoder: 'bow', 'rnn', or 'transformer'
                  // set to 'pass' to pass through word/ELMo embeddings.
                  // TUNE ME: bow or rnn or transformer
sent_combine_method = "max"  // pooling method for encoder states:
                             // 'max', 'mean', or 'final'
bidirectional = 1    // if true, use bidirectional RNN
pair_attn = 1  // 1 to use attn in pair classification/regression tasks
shared_pair_attn = 0  // if true, share pair attn for pairwise tasks
d_hid = 1024          // hidden dimension size (usually num_heads * d_proj for transformer)
                     // TUNE ME:
                     //   BiLSTM: 1500
                     //   Transformer: hid768/heads12 or hid512/heads8 or hid384/heads6
d_hid_attn = 512     // post-attention LSTM state size
d_proj = 512         // task-specific linear project before pooling
                     // TODO: Should be set per task.
n_layers_enc = 2     // number of encoder layers
                     // TUNE ME:
                     //   BiLSTM: 2
                     //   Transformer: 12
skip_embs = 1        // if true, adds skip connection to concatenate encoder
                     // input to encoder output
                     // TUNE ME
sep_embs_for_skip = 0   // whether the skip embedding uses the same embedder
                        // as the original embedding (before skip). Only makes a
                        // difference if we are using ELMo weights. Set to 1
                        // for separate ELMo scalars
n_layers_highway = 0  // number of highway layers
                      // TODO(alex): Where are these layers in the model?
n_heads = 8  // number of transformer heads
             // TUNE ME: See above.
d_tproj = 64  // transformer projection dimension
d_ff = 2048  // transformer feed-forward dimension
dropout = 0.2  // dropout rate
dropout_embs = ${dropout}   // dropout rate for embeddings, same as above by default
                            // NB: this only applies to char embs (non-elmo)

// Training options
no_tqdm = 1  // if true, disable tqdm progress bar; NB: doesn't work b/c tqdm is removed
track_batch_utilization = 0 // if 1, track % of batch that is pad (for tasks with field 'input1'
trainer_type = "sampling"  // type of trainer: 'sampling'
shared_optimizer = 1  // if true, use same optimizer for all tasks
batch_size = 32   // training batch size
                  // TODO: Adjust as needed for memory.
optimizer = "adam" // optimizer. All valid AllenNLP options are available,
                   // including 'sgd'. 'adam' uses the newer AMSGrad variant.
lr = 0.0001          // initial learning rate
min_lr = 0.000001  // (1e-6) minimum learning rate
max_grad_norm = 5.0  // maximum gradient norm
weight_decay = 0.0000001   // weight decay value
task_patience = 1    // patience in decaying per-task learning rate
scheduler_threshold = 0.0001 // threshold used in deciding when to lower learning rate
lr_decay_factor = 0.5  // learning rate decay factor, when validation score
                       // doesn't improve
warmup = 4000  // number of warmup steps for transformer LR schedule

val_data_limit = 5000 // maximum number of examples to be used during mid-training validations
                      // by default, we use the _first_ 5000 examples from each dev set.
                      // Does not apply to the final validation run at the end of main.py.
val_interval = 1000  // number of passes between validation checks
                    //MUST be divisible by bpp_base
max_vals = 1000     // maximum number of validation checks
bpp_base = 1       // number of batches to train per sampled task
patience = 5  // patience in early stopping
keep_all_checkpoints = 0 // If set, keep checkpoints from every validation.
                         // Otherwise, keep only best and (if different) most recent.

// Multi-task training options
weighting_method = "proportional"  // weighting method for sampling:
                                   // 'uniform' or 'proportional' or 'proportional_log'
scaling_method = "max"   // method for scaling loss:
                         // 'min', 'max', 'unit', or 'none'

// Evaluation options
eval_val_interval = 500  // validation interval for eval task
                         // MUST be divisible by bpp_base
                         // TODO: Tune this per task. 500 is probably high for
                         // RTE and low for QQP
eval_max_vals = 1000     // maximum number of validation checks for eval task
write_preds = "test"     // comma-separated list of splits in
                         // {'train', 'val', 'test'} for which we should write
                         // predictions. Will likely break for non-GLUE tasks.
write_strict_glue_format = 0  // if true, write_preds will only write the
                              // 'index' and 'prediction' columns for GLUE
                              // tasks.


// Tasks specific stuff: either task-specific model parameters or eval-time trainer params
// Important note: Model params apply during train and eval phases; trainer params like LR
// apply only during eval phase.
rte = {}
rte_classifier_hid_dim = 128
rte_d_proj = 128
rte_classifier_dropout = 0.4
rte_pair_attn = 0
rte_val_interval = 100
rte_lr = 0.0003

wnli = {}
wnli_classifier_hid_dim = 128
wnli_d_proj = 128
wnli_classifier_dropout = 0.4
wnli_pair_attn = 0
wnli_val_interval = 100
wnli_lr = 0.0003

mrpc = {}
mrpc_classifier_hid_dim = 256
mrpc_d_proj = 256
mrpc_classifier_dropout = 0.2
mrpc_pair_attn = 0
mrpc_val_interval = 100
mrpc_lr = 0.0003

sst = {}
sst_classifier_hid_dim = 256
sst_d_proj = 256
sst_classifier_dropout = 0.2
sst_val_interval = 100
sst_lr = 0.0003

cola = {}
cola_classifier_hid_dim = 256
cola_d_proj = 256
cola_classifier_dropout = 0.2
cola_val_interval = 100
cola_lr = 0.0003

sts-b = {}
sts-b_classifier_hid_dim = 512
sts-b_classifier_dropout = 0.2
sts-b_pair_attn = 1
sts-b_val_interval = 1000
sts-b_lr = 0.0003

qnli = {}
qnli_classifier_hid_dim = 512
qnli_classifier_dropout = 0.2
qnli_pair_attn = 1
qnli_val_interval = 1000
qnli_lr = 0.0003

mnli = {}
mnli_classifier_hid_dim = 512
mnli_classifier_dropout = 0.2
mnli_pair_attn = 1
mnli_val_interval = 1000
mnli_lr = 0.0003

qqp = {}
qqp_classifier_hid_dim = 512
qqp_classifier_dropout = 0.2
qqp_pair_attn = 1
qqp_val_interval = 1000
qqp_lr = 0.0003

nli-prob {
  probe_path = ""
}

// Edge-probing template; not used for any single task,
// but extended per-task below.
edges-tmpl {
    classifier_loss_fn = "sigmoid"  // 'sigmoid' or 'softmax'
    classifier_span_pooling = "attn"  // 'attn' or 'x,y'
    classifier_hid_dim = 256
    classifier_dropout = 0.3
    pair_attn = 0

    // Default iters; might want to change for subtasks after we do tuning.
    max_vals = 10  // TODO: increase, this is << 1 epoch for many tasks
    val_interval = 500
}

edges-srl-conll2005 = ${edges-tmpl}
edges-spr2 = ${edges-tmpl}

edges-dpr = ${edges-tmpl}
edges-coref-ontonotes = ${edges-tmpl}
edges-ner-conll2003 = ${edges-tmpl}

edges-dep-labeling = ${edges-tmpl}
edges-constituent-ptb = ${edges-tmpl}
edges-ccg-tag = ${edges-tmpl}
edges-ccg-parse = ${edges-tmpl}

mnli-diagnostic { use_classifier = "mnli" }

