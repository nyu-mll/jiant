// Default config file.
// Set values here, then override them in custom configs by including this at
// the top, e.g.
// my_expt.conf:
//   include "defaults.conf"
//
//   exp_name = "my_expt"
//   run_name = "run1"
//   n_layers_env = 2
//   word_embs = "fastText"
//

// Logistics
cuda = 0  // gpu id, or -1 to disable
random_seed = 19  // (global) random seed

// Paths and logging
exp_name = ""  // experiment name
run_name = ""  // run name
data_dir = ${JIANT_DATA_DIR}  // required
exp_dir = ${JIANT_PROJECT_PREFIX}"/"${exp_name}"/"  // required
run_dir = ${JIANT_PROJECT_PREFIX}"/"${exp_name}"/"${run_name}  // required
log_file = "log.log"  // log file, goes in run directory

// Execution control
do_train = 1    // run training steps
do_eval = 1     // run eval steps
load_model = 1  // if true, restore main training from checkpoint if available
load_eval_checkpoint = "None"  // path to eval checkpoint, or "None"
reload_tasks = 0     // if true, force reloading tasks
reload_indexing = 0  // if true, force reindexing tasks
reload_vocab = 0     // if true, force vocabulary rebuild

// Tasks and task-specific modules
train_tasks = "glue"  // required, comma-separated list of training tasks,
                  // or 'all' or 'none'
eval_tasks = "glue"   // required, additional eval tasks
train_for_eval = 1  // if true and if needed, train new classifiers for eval tasks
classifier = "mlp"    // classifier type
classifier_hid_dim = 256  // classifier width
classifier_dropout = 0.2  // classifier dropout rate
d_hid_dec = 300           // decoder hidden size
n_layers_dec = 1          // decoder number of layers

// Preprocessing options
max_seq_len = 40         // maximum (input) sequence length
max_word_v_size = 30000  // maximum word vocab size
max_char_v_size = 250    // maximum char vocab size

// Embedding options
word_embs = "none"  // type of embeddings: 'none', 'scratch', 'glove, 'fastText'
word_embs_file = ${WORD_EMBS_FILE}  // path to embeddings file
                                    // shouldn't this be $FASTTEXT_EMBS_FILE ?
fastText = 0                                  // if true, use fastText model
fastText_model_file = ${FASTTEXT_MODEL_FILE}  // path to fastText model
d_word = 300  // dimension of word embeddings
d_char = 100  // dimension of char embeddings
n_char_filters = 100  // number of filters in char CNN
char_filter_sizes = "2,3,4,5"  // size of char filters
elmo = 1             // if true, use ELMo
elmo_chars_only = 1  // if true, use only char CNN layer of ELMo
cove = 0             // if true, use CoVe
char_embs = 0        // if true, use char embeddings
dropout_embs = 0.2   // dropout rate for embeddings
preproc_file = "preproc.pkl"  // file to save preprocessing data
                              // TODO(alex): document what goes in here?

// Model options
sent_enc = "rnn"  // type of sentence encoder: 'bow', 'rnn', or 'transformer'
sent_combine_method = "max"  // pooling method for encoder states:
                             // 'max', 'mean', or 'final'
shared_pair_enc = 1  // if true, share pair encoder for pairwise tasks
bidirectional = 1    // if true, use bidirectional RNN
pair_enc = "simple"  // type of pair encoder: 'simple' or 'attn'
d_hid = 512          // hidden dimension size
n_layers_enc = 2     // number of encoder layers
skip_embs = 1        // if true, adds skip connection to concatenate encoder
                     // input to encoder output
n_layers_highway = 1  // number of highway layers
                      // TODO(alex): Where are these layers in the model?
n_heads = 8  // number of transformer heads
d_proj = 64  // transformer projection dimension
d_ff = 2048  // transformer feed-forward dimension
dropout = 0.2  // dropout rate

// Training options
no_tqdm = 1  // if true, disable tqdm progress bar
trainer_type = "sampling"  // type of trainer: 'sampling' or 'mtl'
shared_optimizer = 1  // if true, use same optimizer for all tasks
batch_size = 64   // training batch size
optimizer = "adam" // optimizer. All valid AllenNLP options are available,
                   // including 'sgd'. 'adam' uses the newer AMSGrad variant.
n_epochs = 10000  // number of epochs to train for
lr = 0.001          // initial learning rate
min_lr = 0.000001  // (1e-5) minimum learning rate
max_grad_norm = 5.0  // maximum gradient norm
weight_decay = 0.0000001   // weight decay value
task_patience = 5    // patience in decaying per-task learning rate
scheduler_threshold = 0.0001 // threshold used in deciding when to lower learning rate
lr_decay_factor = 0.5  // learning rate decay factor, when validation score
                       // doesn't improve
warmup = 4000  // number of warmup steps for transformer LR schedule

// Multi-task training options
val_interval = 500  // number of passes between validation checks
max_vals = 100     // maximum number of validation checks
bpp_base = 1       // number of batches to train per sampled task
weighting_method = "proportional"  // weighting method for sampling:
                              // 'uniform' or 'proportional'
scaling_method = "none"  // method for scaling loss:
                         // 'min', 'max', 'unit', or 'none'
patience = 10  // patience in early stopping

// Evaluation options
eval_val_interval = 1000  // validation interval for eval task
eval_max_vals = 100       // maximum number of validation checks for eval task
write_preds = 0  // if true, write test predictions

