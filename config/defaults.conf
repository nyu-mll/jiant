// Logistics
cuda = 0
random_seed = 19

// Paths and logging
exp_name = ""
run_name = ""
log_file = "log.log"
data_dir = ${JIANT_DATA_DIR}  // required
exp_dir = ${JIANT_PROJECT_PREFIX}"/"${exp_name}"/"  // required
run_dir = ${JIANT_PROJECT_PREFIX}"/"${exp_name}"/"${run_name}  // required

// Execution control
should_train = 1
load_model = 1
force_load_epoch = -1
reload_tasks = 0
reload_indexing = 0
reload_vocab = 0

// Tasks and task-specific modules
train_tasks = ""  // required
eval_tasks = ""   // required
train_for_eval = 1
classifier = "log_reg"
classifier_hid_dim = 512
classifier_dropout = 0.0
d_hid_dec = 300
n_layers_dec = 1

// Preprocessing options
max_seq_len = 40
max_word_v_size = 30000
max_char_v_size = 250

// Embedding options
word_embs = "fastText"
word_embs_file = ${WORD_EMBS_FILE}  // shouldn't this be $FASTTEXT_EMBS_FILE ?
fastText = 0
fastText_model_file = ${FASTTEXT_MODEL_FILE}
d_word = 300
d_char = 100
n_char_filters = 100
char_filter_sizes = "2,3,4,5"
elmo = 0
deep_elmo = 0
cove = 0
char_embs = 0
dropout_embs = 0.2
preproc_file = "preproc.pkl"

// Model options
sent_enc = "rnn"
sent_combine_method = "max"
shared_pair_enc = 1
bidirectional = 1
pair_enc = "simple"
d_hid = 512
n_layers_enc = 1
n_layers_highway = 1
n_heads = 8
d_proj = 64
d_ff = 2048
dropout = 0.2

// Training options
no_tqdm = 0
trainer_type = "sampling"
shared_optimizer = 1
batch_size = 64
optimizer = "sgd"
n_epochs = 10
lr = 1.0
min_lr = 0.00001  // 1e-5
max_grad_norm = 5.0
weight_decay = 0.0
task_patience = 0
scheduler_threshold = 0.0
lr_decay_factor = 0.5
warmup = 4000

// Multi-task training options
val_interval = 10
max_vals = 100
bpp_base = 1
weighting_method = "uniform"
scaling_method = "none"
patience = 5

// Evaluation options
eval_val_interval = 1000
eval_max_vals = 100
write_preds = 1

