// Run edge probing models over BERT,
// without training an encoder on pre-training tasks.
//
// Use this for baseline probing & hyperparameter tuning for probing models.
//
// You should override bert_model_name with one of the valid BERT models, as
// defined in https://github.com/huggingface/pytorch-pretrained-BERT
// For example:
//    bert-base-uncased
//    bert-large-uncased
//    bert-base-cased   (recommended for NER)
//    bert-large-cased  (recommended for NER)

// This imports the defaults, which can be overridden below.
include "../defaults.conf"  // relative path to this file

exp_name = ""  // configure this
run_name = "run"  // default

pretrain_tasks = ""  // empty: don't run main training phase
target_tasks = ""   // train classifier only

// Eval will use task-specific params.
do_pretrain = 0        // skip main train phase
allow_untrained_encoder_parameters = 1  // allow skipping training phase
allow_missing_task_map = 1  // ignore missing classifier_task_map.json
do_target_task_training = 1  // train using eval task params
do_full_eval = 1
write_preds = "val,test"

lr_patience = 5  // vals until LR decay
patience = 20      // vals until early-stopping

bert_model_name = ""
tokenizer = ${bert_model_name}
max_seq_len = 512
cove = 0
word_embs = "none"
elmo = 0

// Use no-op encoder (no params).
sent_enc = "none"
skip_embs = 1  // forward embeddings from lower level.
sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones.
