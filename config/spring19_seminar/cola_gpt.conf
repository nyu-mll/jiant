// Train and run openai gpt model on cola-analysis dataset 

include "cola_elmo.conf"


// Training setting

do_target_task_training = 0  // only do main training phase, since our pretrain and target task are the same
allow_reuse_of_pretraining_parameters = 1  // set this to 1 because we pretrain and finetune model on the same task
batch_size = 2  // keep it small to avoid out-of-memory

// Model setting

elmo = 0
tokenizer = "OpenAI.BPE"  // gpt must use this tokenizer
openai_transformer = 1
openai_embedding_mode = "none"  // only use repr from the last layer
transfer_paradigm = "finetune"  // fine tune the transformers in gpt 
sent_enc = "none"  // we feed the gpt repr into the mlp classifier, no more layer in between 
sep_embs_for_skip = 1  // otherwise some assert will refuse to build the model
lr = 1e-5  // we need to keep lr small for fine-tuning gpt, especially so because our BS is smaller than that in gpt paper
dropout = 0.1  // following original gpt paper
