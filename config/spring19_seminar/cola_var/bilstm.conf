// Import the defaults using the relative path
include "../final.conf"


// Output path
project_dir = ${JIANT_PROJECT_PREFIX}


// Optimization
batch_size = 16
dropout = 0.1 // following BERT paper
lr = 2e-5  // following Jason, Alex
max_epochs = 3 // If positive, maximum number of epochs


// Target tasks
do_target_task_training = 1  // If true, after do_pretrain train the task-specific model parameters
write_preds = "val,test"  // 0 for none, or comma-separated splits in {"train", "val", "test"} 
                          // for which predictions are written to disk during do_full_eval


// Pretraining tasks
load_model = 0  // If true, restore from checkpoint when starting do_pretrain


// Models
// Model, biLSTM
tokenizer = "MosesTokenizer" // The default tokenizer
sent_enc = "rnn"  // "bow", "rnn" for LSTM, "null"
bidirectional = 1
word_embs = "glove"
sep_embs_for_skip = 0 // Skip embedding uses the same embedder object as the original embedding (before skip)
elmo = 0
elmo_chars_only = 0