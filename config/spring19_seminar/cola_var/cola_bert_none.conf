// Import the defaults using the relative path
include "../defaults.conf"


// Output path
project_dir = ${JIANT_PROJECT_PREFIX}
exp_name = exp_pretrain
run_name = run_bert_none


// Logistics
cuda = 0
random_seed = 42


// Rebuilding
allow_reuse_of_pretraining_parameters = 0 // If this is 1 OR do_target_task_training == 0 then 
                                          // `task_names_to_avoid_loading` will be empty; see `main.py` line 271, line 345
reload_tasks = 0     // If true, force the rebuilding of the task files in the experiment directory
reload_indexing = 0  // If true, force the rebuilding of the index files in preproc
reload_vocab = 0     // If true, force the rebuilding of the vocabulary files in the experiment directory
reindex_tasks = ""


// Optimization
lr = 3e-4  // this performs reasonablely well in practice
lr_patience = 4  // number of epochs between last validation improvement and lr annealing, following final.conf
patience = 20  // number of epochs between last validation improvement and early stopping, following final.conf
max_vals = 10000 // following final.conf


// Input Handling
word_embs = "none"
word_embs_file = ""
fastText = 0
fastText_model_file = ""
skip_embs = 1 // concatenate encoder-input (ELMo or embeddings) with the encoder-output



// Target tasks
do_target_task_training = 1  // If true, after do_pretrain train the task-specific model parameters
do_full_eval = 1     // If true, evaluate the model on the tasks on target_tasks
target_tasks = "cola"  // Target tasks given in the same format as pretrain_tasks
write_preds = "val,test"  // 0 for none, or comma-separated splits in {"train", "val", "test"} 
                          // for which predictions are written to disk during do_full_eval


// Pretraining tasks
load_model = 0  // If true, restore from checkpoint when starting do_pretrain
do_pretrain = 0
allow_untrained_encoder_parameters = 1  // Set for experiments involving random un(-pre-)trained encoders only.
pretrain_tasks = "none" // Comma-separated list of pretraining tasks or "glue" or "none";
                                               // If there are multiple entries, the list should contain no spaces, and quotation marks are needed;




// Model

// Models, BERT
batch_size = 24
tokenizer = "bert-large-uncased"
sent_enc = "null" // "bow", "rnn" for LSTM, "null"
transfer_paradigm = "finetune" // "frozen" or "finetune"
bert_fine_tune = 1
bert_model_name = "bert-large-uncased"  // If nonempty, use this BERT model for representations.
                                        // Available values: bert-base-uncased, bert-large-cased, ...
bert_embeddings_mode = "none"  // How to handle the embedding layer of the BERT model:
                               // "none" for only top-layer activation,
                               // "cat" for top-layer concatenated with lexical layer
                               // "only" for only lexical layer
                               // "mix" uses ELMo-style scalar mixing
sep_embs_for_skip = 1 // Skip embedding uses the same embedder object as the original embedding (before skip)                               
elmo = 0
elmo_chars_only = 0
openai_transformer = 0
