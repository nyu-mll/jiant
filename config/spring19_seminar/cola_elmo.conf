// Train and run elmo model on cola-analysis dataset

include "../defaults.conf"


// Do not change

random_seed = 42
word_embs_file = ""  // we do not need this in elmo, gpt or bert
fastText_model_file = ""  // we do not need this in elmo, gpt or bert

// Paths

exp_name = debug  // avoid messing up other experiment records
run_name = debug  // override exp_name and run_name when training new models

// Data setting

reload_vocab = 1
reload_tasks = 1
reload_indexing = 1
reindex_tasks = cola-analysis

// Training setting

load_model = 0

pretrain_tasks = cola-analysis  // cola-analysis contains the full training set of cola
                                // we can change this to cola, when sharing classifier between tasks become available
do_pretrain = 1  // train the model on top of embedding layer

target_tasks = cola-analysis
do_target_task_training = 1  // tune the parameters on target task
do_full_eval = 1  // evaluation the model after training

lr = 3e-4  // this performs reasonablely well in practice
allow_untrained_encoder_parameters = 0  // set this to 1 when running random-elmo
allow_reuse_of_pretraining_parameters = 1  // set this to 1 because we pretrain and finetune model on the same task
lr_patience = 4  // number of epochs between last validation improvement and lr annealing, following final.conf
patience = 20  // number of epochs between last validation improvement and early stopping, following final.conf
max_vals = 10000 // following final.conf

// Eval setting

write_preds = 'test'  // you can upload the test set prediction for evaluation

// Model settings

elmo = 1
elmo_chars_only = 0  // use full elmo
sep_embs_for_skip = 1
