// Train and run openai gpt model on cola-analysis dataset 

include "../demo.conf"




exp_name = debug2  // avoid messing up other experiment records
run_name = debug2

batch_size = 2  // keep it small to avoid out-of-memory

//do_pretrain = 1
//do_target_task_training = 0
// Model setting

elmo = 0
tokenizer = "OpenAI.BPE"  // gpt must use this tokenizer
openai_transformer = 1
sep_embs_for_skip = 1  // otherwise some assert will refuse to build the model
