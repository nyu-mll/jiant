// Import the defaults using the relative path
include "../defaults.conf"


// Optimization
batch_size = 16
dropout = 0.1 // following BERT paper
val_interval = 100
target_train_max_vals = 100
mnli_val_interval = 100
patience = 40
lr_patience = 20
lr = 3e-4
cola_lr = 3e-4
max_epochs = 20


// Target tasks
write_preds = "val,test"  // 0 for none, or comma-separated splits in {"train", "val", "test"} 
                          // for which predictions are written to disk during do_full_eval


// Pretraining tasks
load_model = 0  // If true, restore from checkpoint when starting do_pretrain


// Models
// Model, bow + GloVe
tokenizer = "MosesTokenizer" // The default tokenizer
sent_enc = "bow" // "bow", "rnn" for LSTM, "none"
word_embs = "glove"
skip_embs = 0
