// Import the defaults using the relative path
include "../final.conf"


// Optimization
batch_size = 16
dropout = 0.1 // following BERT paper
lr = 2e-5  // following Jason, Alex
cola_lr = 2e-5
max_epochs = 3 // If positive, maximum number of epochs
val_interval = 100
target_train_max_vals = 100
mnli_val_interval = 100


// Target tasks
write_preds = "val,test"  // 0 for none, or comma-separated splits in {"train", "val", "test"} 
                          // for which predictions are written to disk during do_full_eval


// Pretraining tasks
load_model = 0  // If true, restore from checkpoint when starting do_pretrain


// Models
// Model, BERT
tokenizer = "bert-large-cased"
sent_enc = "none" // "bow", "rnn" for LSTM, "none"
transfer_paradigm = "finetune" // "frozen" or "finetune"
bert_fine_tune = 1
bert_model_name = "bert-large-cased"  // If nonempty, use this BERT model for representations.
                                        // Available values: bert-base-uncased, bert-large-cased, ...
bert_embeddings_mode = "none"  // How to handle the embedding layer of the BERT model:
                               // "none" for only top-layer activation,
sep_embs_for_skip = 1 // Skip embedding uses the same embedder object as the original embedding (before skip)                               
elmo = 0
elmo_chars_only = 0
