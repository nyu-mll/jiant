// Run edge probing models over BERT,
// without training an encoder on pre-training tasks.
//
// Use this for baseline probing & hyperparameter tuning for probing models.
//
// You should override bert_model_name with one of the valid BERT models, as
// defined in https://github.com/huggingface/pytorch-pretrained-BERT
// For example:
//    bert-base-uncasedg
//    bert-large-uncased
//    bert-base-cased   (recommended for NER)
//    bert-large-cased  (recommended for NER)

// This imports the defaults, which can be overridden below.
include "../defaults.conf"  // relative path to this file

project_dir = ${JIANT_PROJECT_PREFIX}
exp_name = "bert_gap"  // configure this
run_name = "run"  // default

pretrain_tasks = "gap-coreference"  // empty: don't run main training phase
target_tasks = "gap-coreference"   // train classifier only

// Eval will use task-specific params.
do_pretrain = 0        // skip main train phase
allow_untrained_encoder_parameters = 1  // allow skipping training phase
allow_missing_task_map = 1  // ignore missing classifier_task_map.json
do_target_task_training = 1  // train using eval task params
do_full_eval = 1
write_preds = "val,test"

reload_tasks = 1
reload_vocab = 1
reload_index = 1
reindex_tasks = "gap-coreference"
lr_patience = 5  // vals until LR decay
load_model = 0 
bert_model_name = "bert-base-cased"
tokenizer = ${bert_model_name}
bert_fine_tune = 1
batch_size = 8
max_seq_len = 511
cove = 0
word_embs = "none"
elmo = 0

// Use no-op encoder (no params).
sent_enc = "null"
// Use no-op encoder (no params).
sent_enc = "null"
skip_embs = 1  // forward embeddings from lower level. 
sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones. 
classifier_loss_fn = "sigmoid"




