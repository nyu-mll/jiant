include "defaults.conf"

 // _All_ final runs will share preproc and tasks.
max_seq_len = 80
tokenizer = "bert-base-cased"
bert_model_name = "bert-base-cased"
exp_name = "bert-base-cased"
bert_embeddings_mode = "top"   // how to use the outputs of the BERT module
                                // set as "top", we use only the top-layer activation
                                // other options: "only" uses the lexical layer (first layer)
                                //                "cat" uses lexical layer + top layer
bert_fine_tune = 1
elmo = 0
elmo_chars_only = 0 
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
classifier = log_reg // following BERT paper

max_epochs_per_task = 3
transfer_paradigm = finetune
do_target_task_training = 1
do_full_eval = 1

dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 16
max_epochs_per_task = 3
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

 // For all generation tasks.
max_word_v_size = 20000
max_targ_word_v_size = 20000